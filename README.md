# NBA-preds-transformer

## 1 Introduction
Deep learning models in [sports analytics](https://en.wikipedia.org/wiki/Sports_analytics) generally explore specific player actions to predict the outcome of games. There is a lack of research into predictive models for longer term player performance and outcomes. For instance, player careers in the NBA have long-term temporal dependencies that can be extracted in terms of statistical features through neural networks.  

In this project, we aim to generate monthly projections for a chosen set of statistics for the next 2 seasons based on player performance in the previous 2 seasons they have played in. The statistics we use and predict are unaffected by team statistics and based solely on player performance. Although it is possible to use varied length input by entering the player’s full career, the lack of data justifies [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation) and [windowing](https://www.geeksforgeeks.org/window-sliding-technique/) with fixed length inputs, as explored further in the data processing section.

[Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#:~:text=A%20transformer%20is%20a%20deep,and%20computer%20vision%20(CV).) have shown great promise recently over [RNN-LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) in sequence-to-sequence prediction models through the introduction of [self-attention](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). We use transformers for monthly time-series forecasting for each player with a set of 12 statistics. Our model makes use of the complex nature of the data in terms of feature dimensions to achieve realistic predictions of player growth and career potential.

## 2 Illustration
something something

## 3 Background and Related Work
Most previous work concerned with player performance in sports has frequently looked at short-term outcomes such as game results. However, predictive models that project statistics for a longer period of time have generally applied RNN-LSTM approaches. In comparison, the transformer model has shown significantly better performance for time-series forecasting problems through mitigating long dependency issues with self-attention where each time step has direct access to all previous time steps.

Benavidez et al., 2019 [1], researchers at Stanford, look at a similar player performance projection model in baseball as opposed to basketball. The multivariate LSTM model, the best performing model in the paper, learns yearly offensive “hitting” metrics for baseball players to predict the OPS (on-base plus slugging) for the next year. Other approaches are linear regression and SVM models that fail to consider dependencies from one time step to another. The loss metric used is MSE, similar to the model we implement. The paper concludes that the poor performance of the models is due to insufficient features and discusses that improvements could build on different player career trends. Based on this, we incorporate statistics that are more closely related with each other and remove redundant features. Additionally, the nature of the problem involves predicting the future, which is technically impossible but developments on existing methods can have more accurate player projections. 
The paper by Devarapalli et al., 2018 [2] explores game-to-game NBA player performance using deep learning and more closely relates to our problem. The report explores a RNN-LSTM model that takes in 18 player statistics for 10 games in the 2016-17 season to predict a score reflecting a player’s value in fantasy NBA. The results are enhanced by using a neural network on top of the LSTM model to provide context to predictions. The paper recommends consideration of more training data including previous season data in future work, which we accommodate in our model.
